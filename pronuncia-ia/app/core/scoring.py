import json
_lev_source = None
try:
    # Prefer python-Levenshtein (fast C implementation)
    from Levenshtein import distance as lev
    _lev_source = "python-Levenshtein"
except Exception:
    try:
        # Fallback to rapidfuzz if available
        from rapidfuzz.distance import Levenshtein as _rlev
        def lev(a, b):
            return _rlev.distance(a, b)
        _lev_source = "rapidfuzz"
    except Exception:
        # Final fallback: use difflib (pure-Python, slower and returns ratio -> convert to distance)
        import difflib
        def lev(a, b):
            if not a and not b:
                return 0
            ratio = difflib.SequenceMatcher(None, a, b).ratio()
            # Convert similarity ratio to an integer distance approximating Levenshtein
            return int(round((1.0 - ratio) * max(len(a), len(b))))
        _lev_source = "difflib"

print(f"[DEBUG] Using Levenshtein implementation: {_lev_source}")

import os
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente do arquivo .env.
# Tentamos m√∫ltiplos locais para facilitar diferentes formas de executar o servidor
# 1) ./pronuncia-ia/.env (diret√≥rio do servi√ßo)
# 2) ./c317---IA/.env (repo root - um n√≠vel acima)
# 3) cwd/.env (current working dir)
possible_envs = [
    Path(__file__).parent.parent.parent / ".env",  # pronuncia-ia/.env
    Path(__file__).parent.parent.parent.parent / ".env",  # repo root .env
    Path.cwd() / ".env",
]

env_path = None
for p in possible_envs:
    try:
        if p.exists():
            env_path = p
            break
    except Exception:
        continue

if env_path:
    load_dotenv(dotenv_path=env_path)
    print(f"[DEBUG] Carregado .env a partir de: {env_path}")
else:
    print(f"[DEBUG] Nenhum .env encontrado em: {[str(p) for p in possible_envs]}")

# Import dos modelos de chat
models_path = Path(__file__).parent.parent.parent / "models"
sys.path.insert(0, str(models_path))

print(f"[DEBUG] üìÅ Path para models: {models_path}")
print(f"[DEBUG] üìÅ Arquivo modelos.py existe: {(models_path / 'modelos.py').exists()}")

try:
    from modelos import OpenAIChat, GeminiChat
    print(f"[DEBUG] ‚úÖ Import dos modelos bem sucedido!")
    print(f"[DEBUG] OpenAIChat: {OpenAIChat}")
    print(f"[DEBUG] GeminiChat: {GeminiChat}")
except ImportError as e:
    print(f"[DEBUG] ‚ùå Erro ao importar modelos: {e}")
    import traceback
    traceback.print_exc()
    OpenAIChat = None
    GeminiChat = None

def _norm(s: str) -> str:
    return "".join(ch.lower() for ch in s.strip() if ch.isalnum() or ch == " ")

def string_similarity(expected: str, predicted: str) -> float:
    """C√°lculo de similaridade usando Levenshtein (m√©todo tradicional)"""
    a, b = _norm(expected), _norm(predicted)
    if not a and not b:
        return 1.0
    d = lev(a, b)  # Dist√¢ncia de Levenshtein
    return 1.0 - d / max(len(a), len(b))

# ============================================================================
# M√âTODOS ALTERNATIVOS TESTADOS (Modelos Pr√©-treinados Especializados)
# ============================================================================
# Durante o desenvolvimento, testamos diferentes abordagens para avalia√ß√£o:
#
# 1. An√°lise Ac√∫stica com Wav2Vec2:
#    - Modelo especializado em reconhecimento de padr√µes de √°udio
#    - Vantagem: Melhor para detectar nuances de pron√∫ncia em portugu√™s
#    - Desvantagem: Requer muito processamento e an√°lise de features ac√∫sticas
#
# def pronunciation_score_wav2vec2(expected: str, predicted: str, audio_features) -> dict:
#     """
#     Avalia√ß√£o usando features ac√∫sticas do Wav2Vec2.
#     Analisa diretamente as caracter√≠sticas do √°udio al√©m da transcri√ß√£o.
#     """
#     from models.modelos import Wav2Vec2
#     model = Wav2Vec2()
#     # An√°lise de features ac√∫sticas (MFCCs, pitch, energia)
#     # acoustic_score = analyze_acoustic_features(audio_features)
#     # phonetic_accuracy = compare_phonemes(expected, predicted)
#     # return combined_score
#     pass
#
# 2. An√°lise com Faster Whisper (otimizado):
#    - Vers√£o otimizada do Whisper com melhor performance
#    - Vantagem: Mais r√°pido que Whisper original
#    - Desvantagem: Ainda focado em transcri√ß√£o, n√£o em avalia√ß√£o pedag√≥gica
#
# def pronunciation_score_faster_whisper(expected: str, audio_path: str) -> dict:
#     """
#     Usa Faster Whisper para transcri√ß√£o e an√°lise de confian√ßa.
#     """
#     from models.modelos import FasterWhisper
#     model = FasterWhisper()
#     # segments com scores de confian√ßa por palavra
#     # confidence_scores = get_word_confidence(segments)
#     # return detailed_analysis
#     pass
#
# 3. Ensemble de Modelos:
#    - Combinar m√∫ltiplos modelos STT para consenso
#    - Vantagem: Mais robusto, reduz erros individuais
#    - Desvantagem: 3-5x mais lento, complexidade aumentada
#
# def pronunciation_score_ensemble(expected: str, audio_path: str) -> dict:
#     """
#     Combina resultados de m√∫ltiplos modelos para consenso.
#     """
#     # whisper_result = Whisper().transcribe(audio_path)
#     # wav2vec_result = Wav2Vec2().transcribe(audio_path)
#     # faster_whisper_result = FasterWhisper().transcribe(audio_path)
#     # consensus = voting_mechanism([whisper, wav2vec, faster])
#     # return aggregate_score(consensus)
#     pass
#
# CONCLUS√ÉO DA AN√ÅLISE:
# Optamos por usar LLMs (GPT/Gemini) pois oferecem:
# ‚úÖ Feedback qualitativo rico e pedag√≥gico (n√£o apenas num√©rico)
# ‚úÖ Identifica√ß√£o contextual de erros (entende o "porqu√™")
# ‚úÖ Sugest√µes personalizadas de melhoria
# ‚úÖ An√°lise lingu√≠stica al√©m da similaridade textual
# ‚úÖ Menor complexidade de implementa√ß√£o
# ============================================================================

def pronunciation_score(expected: str, predicted: str) -> dict:
    """
    Calcula a pontua√ß√£o de pron√∫ncia baseado na similaridade entre a palavra-alvo e o texto reconhecido.
    M√âTODO TRADICIONAL (Levenshtein) - usado como fallback.
    """
    sim = string_similarity(expected, predicted)  # 0..1
    hit = 1.0 if _norm(expected) == _norm(predicted) else 0.0  # Verifica se √© uma correspond√™ncia exata
    score = 0.8 * sim + 0.2 * hit  # A pontua√ß√£o final, ponderando a similaridade e o hit
    return {
        "score": round(100 * score, 1),
        "similarity": round(100 * sim, 1),
        "hit": bool(hit),
        "predicted": predicted,
        "feedback": "Avalia√ß√£o autom√°tica baseada em similaridade textual.",
        "method": "levenshtein"
    }

def pronunciation_score_with_ai(expected: str, predicted: str, provider: str = "openai", language: str = "portugu√™s") -> dict:
    """
    Avalia pron√∫ncia usando GPT/Gemini para an√°lise qualitativa detalhada.
    
    Args:
        expected: Palavra/frase que deveria ser falada
        predicted: O que foi realmente transcrito
        provider: "openai" ou "gemini"
        language: Idioma para contextualizar a avalia√ß√£o
    
    Returns:
        dict com score, feedback detalhado, sugest√µes, etc.
    """
    
    print(f"\n[DEBUG] üöÄ Iniciando avalia√ß√£o com IA")
    print(f"[DEBUG] Provider: {provider}")
    print(f"[DEBUG] Language: {language}")
    print(f"[DEBUG] Expected: '{expected}'")
    print(f"[DEBUG] Predicted: '{predicted}'")
    
    # Validar se as classes est√£o dispon√≠veis
    print(f"[DEBUG] OpenAIChat dispon√≠vel: {OpenAIChat is not None}")
    print(f"[DEBUG] GeminiChat dispon√≠vel: {GeminiChat is not None}")
    
    if provider.lower() == "openai" and OpenAIChat is None:
        print("[DEBUG] ‚ö†Ô∏è OpenAI n√£o dispon√≠vel, usando m√©todo tradicional")
        return pronunciation_score(expected, predicted)  # Fallback para m√©todo tradicional
    if provider.lower() == "gemini" and GeminiChat is None:
        print("[DEBUG] ‚ö†Ô∏è Gemini n√£o dispon√≠vel, usando m√©todo tradicional")
        return pronunciation_score(expected, predicted)  # Fallback para m√©todo tradicional
    
    # Prompt otimizado para avalia√ß√£o de pron√∫ncia
    prompt = f"""Voc√™ √© um professor de {language} especializado em avalia√ß√£o de pron√∫ncia.

**TAREFA:** Avaliar a pron√∫ncia do aluno comparando o que ele deveria falar com o que realmente foi transcrito.

**Palavra/Frase esperada:** "{expected}"
**O que foi transcrito:** "{predicted}"

**INSTRU√á√ïES:**
1. D√™ uma nota de 0 a 100 considerando:
   - Precis√£o das palavras (70%)
   - Poss√≠veis erros de pron√∫ncia detectados na transcri√ß√£o (20%)
   - Clareza e flu√™ncia (10%)

2. Se a transcri√ß√£o for EXATAMENTE igual ao esperado, d√™ nota 100.

3. Forne√ßa feedback construtivo e espec√≠fico:
   - O que o aluno acertou
   - Quais erros foram cometidos
   - Dicas pr√°ticas para melhorar

4. Se houver erros, identifique quais sons/palavras foram problem√°ticos.

**IMPORTANTE:** Retorne APENAS um JSON v√°lido neste formato exato:
{{
    "score": <n√∫mero de 0 a 100>,
    "match": <true se transcri√ß√£o == esperado, false caso contr√°rio>,
    "feedback": "<feedback detalhado em {language}>",
    "errors": ["<lista de erros espec√≠ficos>"],
    "suggestions": ["<dicas pr√°ticas para melhorar>"],
    "highlights": {{
        "correct": ["<palavras/sons que acertou>"],
        "incorrect": ["<palavras/sons que errou>"]
    }}
}}

N√ÉO adicione texto antes ou depois do JSON. Retorne apenas o objeto JSON."""

    try:
        print(f"[DEBUG] üìù Criando inst√¢ncia do chat {provider}...")
        
        # Chamar o modelo apropriado
        if provider.lower() == "gemini":
            chat = GeminiChat()
            print(f"[DEBUG] ‚úÖ GeminiChat instanciado com sucesso")
        else:  # openai √© o padr√£o
            chat = OpenAIChat()
            print(f"[DEBUG] ‚úÖ OpenAIChat instanciado com sucesso")
        
        print(f"[DEBUG] ü§ñ Enviando prompt para IA...")
        response_text = chat.reply_from_text(prompt, system="Voc√™ √© um avaliador de pron√∫ncia preciso. Sempre retorne JSON v√°lido.")
        print(f"[DEBUG] üì® Resposta recebida (primeiros 200 chars): {response_text[:200]}...")
        
        # Tentar extrair JSON da resposta (alguns modelos podem adicionar markdown)
        response_text = response_text.strip()
        if response_text.startswith("```json"):
            print(f"[DEBUG] üîß Removendo marcador ```json")
            response_text = response_text[7:]
        if response_text.startswith("```"):
            print(f"[DEBUG] üîß Removendo marcador ```")
            response_text = response_text[3:]
        if response_text.endswith("```"):
            print(f"[DEBUG] üîß Removendo marcador ``` do final")
            response_text = response_text[:-3]
        response_text = response_text.strip()
        
        print(f"[DEBUG] üîç Tentando parsear JSON...")
        # Parse do JSON
        result = json.loads(response_text)
        print(f"[DEBUG] ‚úÖ JSON parseado com sucesso!")
        print(f"[DEBUG] Score retornado: {result.get('score')}")
        
        # Garantir que tem todos os campos necess√°rios
        return {
            "score": result.get("score", 0),
            "match": result.get("match", False),
            "predicted": predicted,
            "expected": expected,
            "feedback": result.get("feedback", "Sem feedback dispon√≠vel."),
            "errors": result.get("errors", []),
            "suggestions": result.get("suggestions", []),
            "highlights": result.get("highlights", {"correct": [], "incorrect": []}),
            "method": f"ai-{provider}",
            "language": language
        }
        
    except json.JSONDecodeError as e:
        # Se falhar no parse JSON, retornar m√©todo tradicional
        print(f"[DEBUG] ‚ùå Erro ao parsear JSON da IA: {e}")
        print(f"[DEBUG] Resposta completa: {response_text}")
        fallback = pronunciation_score(expected, predicted)
        fallback["ai_response"] = response_text  # Para debug
        return fallback
        
    except Exception as e:
        # Qualquer outro erro, retornar m√©todo tradicional
        print(f"[DEBUG] ‚ùå Erro ao avaliar com IA: {type(e).__name__}: {e}")
        import traceback
        print(f"[DEBUG] Traceback completo:")
        traceback.print_exc()
        return pronunciation_score(expected, predicted)
